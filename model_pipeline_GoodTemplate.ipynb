{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of this file\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a template for ML workflows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.base import BaseEstimator, TransformerMixin #gives fit_transform method for free\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### collect data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "part 1: collect raw data using SQL\n",
    "\"\"\"\n",
    "#### now this is entire 3 years' data (BETWEEN '2016-01-01'  AND '2018-12-31')\n",
    "\n",
    "\"\"\"\n",
    "# > ** part 1. connect to redshift and SQL data **\n",
    "#     - connect to redshift\n",
    "#     - process sql and get selected data to dataframe\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "path_json = \"/Users/xduan/Documents/work/learning/setup/redshift_creds.json.nogit\"\n",
    "with open(path_json) as fh:\n",
    "    creds = json.load(fh)\n",
    "\n",
    "HOST = creds['host_name']\n",
    "PORT = creds['port_num']\n",
    "USER =  creds['user_name']\n",
    "PASSWORD = creds['password']\n",
    "DATABASE = creds['db_name']\n",
    "\n",
    "# print(\"Getting subscription info\")\n",
    "conn = psycopg2.connect(\n",
    "  host=HOST,\n",
    "  port=PORT,\n",
    "  user=USER,\n",
    "  password=PASSWORD,\n",
    "  database=DATABASE,\n",
    "  )\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM adhoc.subs_monthly_lifetime\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    raw_data = pd.read_sql(query, conn)\n",
    "finally:\n",
    "    conn.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13443, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "part 2.preprocessing data\n",
    "    - get month data as a feature\n",
    "    - parse grouping to multiply columns\n",
    "\"\"\"\n",
    "# convert to date format\n",
    "raw_data['start_date'] = pd.to_datetime(raw_data['start_date'])\n",
    "\n",
    "# get year\n",
    "raw_data['year'] = raw_data['start_date'].dt.year\n",
    "#get month\n",
    "raw_data['month'] = raw_data['start_date'].dt.month\n",
    "\n",
    "# split grouping\n",
    "new = raw_data['grouping'].str.split('-', n = -1, expand = True)\n",
    "raw_data['partner'],raw_data['geo'],raw_data['channel'],raw_data['primary_optin_listen'], raw_data['optin_source'],raw_data['account'] = new[0], new[1], new[2], new[3], new[5], new[6]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choose appropriate dataset for training as data: set a threshold on initial cohort size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_threshold = 100\n",
    "filter_index = raw_data['initial_cohort_size'] > cohort_threshold\n",
    "data = raw_data[filter_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2205, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### part 2  feature engineering\n",
    "   - selected features: \n",
    "   ['month','partner','geo','channel','primary_optin_listen','year','optin_source','account','price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month_str'] = data['month'].apply(lambda row: 'month_'+ str(row))\n",
    "data['year_str'] = data['year'].apply(lambda row: 'year_'+ str(row))\n",
    "\n",
    "# get categorical features\n",
    "category_lst = ['month_str','partner','geo','channel','primary_optin_listen','year_str','optin_source','account']\n",
    "# numerical features\n",
    "num_feature_lst = ['price']\n",
    "# combine them to have final column list\n",
    "column_lst = category_lst + num_feature_lst\n",
    "# get X and y\n",
    "X = data[column_lst]\n",
    "y = data['lifetime']\n",
    "\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Data summary\n",
    "- X: filtered based on initial cohort_size\n",
    "- y: MONTHLY lifetime for each cohort\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### part 3. regression on monthly lifetime\n",
    "    - random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get columns names after one-hot-encoding\n",
    "this is to use for plotting feature importance\n",
    "\"\"\"\n",
    "\n",
    "ohe = ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True)\n",
    "X_dummy_category = ohe.fit_transform(X_train[category_lst])\n",
    "# dummy category column list\n",
    "dummy_column_lst = X_dummy_category.columns.tolist() # features\n",
    "\n",
    "# dummy + numerical columns\n",
    "final_column_lst = dummy_column_lst + num_feature_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned random forest: {'rf__n_estimators': 500, 'rf__max_features': 'sqrt', 'rf__random_state': 1234}\n",
      "Tuned R squared: 0.800343269668\n",
      "Tuned MSE: 0.745392459488\n",
      "this takes 38.32 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "\"\"\"\n",
    "\n",
    "start = time()\n",
    "\"\"\"\n",
    "ColumnTransformer\n",
    "\"\"\"\n",
    "\n",
    "# numeric feature pipeline\n",
    "num_steps = [('scaler', StandardScaler())]\n",
    "num_pipe = Pipeline(num_steps)\n",
    "\n",
    "# categorical feature pipeline\n",
    "#     cat_steps = [('LBL', MultiColumnLabelEncoder())]\n",
    "cat_steps = [('ohe', ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True))\n",
    "            ,('scaler', StandardScaler())]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "# use ColumnTransformer do transformation separately\n",
    "transformers = [('cat', cat_pipe, category_lst),\n",
    "                ('num', num_pipe, num_feature_lst)] # change this list\n",
    "ct = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "final pipeline: ohe+ classifier with manual weight\n",
    "\"\"\"\n",
    "\n",
    "# set up final steps\n",
    "steps = [('transform', ct),\n",
    "         ('rf', RandomForestRegressor())]\n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'rf__random_state': [1234]\n",
    "              ,'rf__n_estimators':[200, 500]\n",
    "              ,'rf__max_features':['auto','sqrt']}\n",
    "\n",
    "gm_cv = GridSearchCV(pipeline, param_grid= parameters, cv = 5)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "evaluation on test dataset\n",
    "\"\"\"\n",
    "# Compute and print the metrics\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned random forest: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned R squared: {}\".format(r2))\n",
    "print(\"Tuned MSE: {}\".format(mse))\n",
    "\n",
    "print(\"this takes %.2f seconds\" %(time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is optional\n",
    "This is to plot feature importance in Random Forest.\n",
    "\n",
    "\"\"\"\n",
    "# function to plot feature_importance or coefficients\n",
    "def plot_feature_importance(feature_importances,feature_names, title):\n",
    "    ftr_imp_df = pd.DataFrame(sorted(zip(feature_names,feature_importances)\n",
    "                          , key=lambda x: x[1], reverse = False)\n",
    "                   )\n",
    "    y_pos = np.arange(ftr_imp_df.shape[0])\n",
    "\n",
    "    plt.barh(y_pos, ftr_imp_df[1], align='center', alpha=0.4)\n",
    "    plt.yticks(y_pos, ftr_imp_df[0])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "plt.subplots(figsize=(15,10))    \n",
    "rf = gm_cv.best_estimator_.named_steps['rf']\n",
    "feature_importances = rf.feature_importances_\n",
    "feature_names = final_column_lst ## it is final_column_lst + direction sign from Ridge\n",
    "\n",
    "\n",
    "plot_feature_importance(feature_importances,feature_names, title = 'random forest for lifetime')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save trained model using pickle, so we can use this trained model for future prediction.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pipe = gm_cv.best_estimator_\n",
    "\n",
    "path = '/Users/xduan/Desktop'\n",
    "dump_path = path + '/pipe.pkl'\n",
    "\n",
    "with open(dump_path, 'wb') as p:\n",
    "    pickle.dump(pipe, p)\n",
    "\n",
    "# 2.Load it again from  le:\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### other good pipeline flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Customizing your pipeline with your own scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "auc_scorer = make_scorer(roc_auc_score)\n",
    "grid_search = GridSearchCV(pipe, param_grid=params, scoring=auc_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- workflow using FunctionTransformer to get self-defined module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "case 3: use FunctionTransformer\n",
    "\n",
    "- copy data instead of directly working on it\n",
    "- treat data as numpy array inside function\n",
    "\"\"\"\n",
    "\n",
    "# def negate_second_column(X):\n",
    "#     Z = X.copy()\n",
    "#     Z[:,1] = -Z[:,1] \n",
    "#     return Z\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#                 ('ft', FunctionTransformer(negate_second_column))\n",
    "#                  , ('clf', RandomForestClassifier())\n",
    "#                 ]\n",
    "#                )\n",
    "\n",
    "# Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):\n",
    "    Z = X.copy()\n",
    "    Z[:,1] = Z[:,1] > multiplier*np.mean(Z[:,1])\n",
    "    return Z\n",
    "\n",
    "# Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Optimize the parameter multiplier using GridSearchCV\n",
    "params = {'ft__multiplier': [1, 2, 3]}\n",
    "gs = GridSearchCV(pipe, param_grid=params)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "with open('pipe.pkl', 'wb') as file: \n",
    "    pickle.dump(gs, file)\n",
    "\n",
    "    \n",
    "# 2.Production environment:\n",
    "with open('pipe.pkl', 'rb') as file:\n",
    "    gs = pickle.dump(gs, file)\n",
    "\n",
    "gs.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
